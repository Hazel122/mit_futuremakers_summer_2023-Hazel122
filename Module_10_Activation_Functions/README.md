# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
* Mathematical definition of Sigmoid, Tanh, ReLU. 
* ReLu solved vanishing gradient problem

## Challenging, interesting, or exciting aspects of today's assignment
The AI-generated meme is fu

## Additional resources used 
chatGPT
