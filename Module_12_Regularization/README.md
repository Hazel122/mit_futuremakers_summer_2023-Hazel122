# Regularization

## Topics covered in today's module
* L1/L2 Regularization
* Dropout
* Overfitting
* Underfitting

## Main takeaways from doing today's assignment
* The simpler model is less likely to be overfitted than a complex model.
* weight regularization: force the model's weight to be small values.
* what L1 and L2 regularization do: they drive the parameters towards small values, zeroes in a perfect case. A sparse model has many parameters zeroed out by regularization. 

## Challenging, interesting, or exciting aspects of today's assignment
Understanding the mathematical cost function is kind of challenging.

## Additional resources used 
chatGPT
